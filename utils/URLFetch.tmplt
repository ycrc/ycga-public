
'''
TODO
need to be able to do a aws copy using my keys, without giving keys away

1) setuid program that runs as me and can access my keys.
2) hide keys in this script, maybe by compiling it?
3) in python, parse ini file, and pass values directly to boto3 (still need to do setuid though)

'''

# keys will be inserted here
KEYSXXXXXXXXXXKEYS

import sys, os, argparse, re, logging, subprocess, datetime


urlpat=re.compile('^http://fcb.ycga.yale.edu:3010/([^/]+)/([^/]+)/?$')
epilog="epilog"

ELData='/gpfs/ycga/project/lsprog/tools/external/data'
ArchiveRoot='/SAY/archive/YCGA-729009-YCGA-A2/archive'

'''
Examples of where to find .deleted files.

/gpfs/ycga/sequencers/pacbio/gw92/10x/Single_Cell/20211207_cad99_3p.deleted
/gpfs/ycga/sequencers/pacbio/gw92/10x/Single_Cell/netid/20211207_cad99_3p.deleted (eventual, none yet)
/gpfs/ycga/sequencers/pacbio/data/PH40-Mosquito7-5D-010716_462.deleted

need to figure out "runname", then look for runname.deleted in appropriate place.
'''


''' Examples of target to tar:
replace /gpfs/ycga/sequencers by /SAY/archive/YCGA-729009-YCGA-A2/archive and append .tar?

/gpfs/ycga/sequencers/pacbio/data/r64146_20211028_192652/
/SAY/archive/YCGA-729009-YCGA-A2/archive/pacbio/data/r64146_20211028_192652.tar

/gpfs/ycga/sequencers/pacbio/gw92/10x/Single_Cell/ha328/20220603_ha328_vdj
/SAY/archive/YCGA-729009-YCGA-A2/archive/pacbio/gw92/10x/Single_Cell/ha328/20220603_ha328_vdj.tar
s3://hpc-rsha-std-mult-000-a2/172.18.4.146/kc/.17712/data/1408/archive/pacbio/gw92/10x/Single_Cell/ha328/20220603_ha328_vdj.tar

'''
def fatal(s):
    logger.error(s)
    sys.exit(1)

'''

The logic here:
1) dereference the URL provided to the staging dir
2) look through all links in the staging dir.  If they are readable, we can just make links.
3) There are 2 reasons the links might not be readable:
  a) the user doesn't have permission in some superdirectory (most likely the 'netid' dir)
  b) the data has been archived and deleted.
In case 1), we want to return a sensible error message
In case 2), we want to look for a tarball in the archive and pull it down

To find the tarball, find the .deleted file, and convert to the bucket location

We'll also check that the user can read the deleted file.  To prevent unauthorized access, we will:
- use group or user acls on all new style netid dirs
- use acls directly on these .deleted files if netid dirs don't exist

'''

def checkForBroken(path):
    ok=True
    for d, ds, fs in os.walk(path, followlinks=True):
        for f in fs:
            path=os.path.join(d, f)
            if os.path.islink(path):
                target=os.readlink(path)
                if not os.path.isabs(target):
                    target=os.path.join(os.path.dirname(path), target)
                if not os.path.exists(target):
                    ok=False
    return ok

'''
Need to handle /gpfs/ycga/sequencers and /ycga-gpfs/sequencers
'''

tenXpat=re.compile(r'^/gpfs/ycga/sequencers/pacbio/gw92/10x/(?:Single_Cell|Exome|Genome)/(?:[a-z]+[0-9]+/)?(\d+_[^/]+)')
PBpat = re.compile(r'^/gpfs/ycga/sequencers/pacbio/data/(r\d+_[^/]+)')

def getTarball(target):
    # first determine run type and run name from link
    mo=tenXpat.match(target)

    if not mo:
        mo=PBpat.match(target)
        
    if mo:
        dfile=mo.group(0)+'.deleted'
        if os.path.exists(dfile) and os.access(dfile, os.R_OK):
            archived=True
            logger.debug(f"found {dfile}")

            tfile=mo.group(0)+'.tar'
            # first try normal location
            objpath=re.sub('(/gpfs/ycga/sequencers|/ycga-gpfs/sequencers)', 's3://hpc-rsha-std-mult-000-a2/172.18.4.146/kc/.17712/data/1408/archive', tfile)
            ret=downloadObj(objpath)
            if ret==0: return ret

            # next try artico location
            objpath=re.sub('(/gpfs/ycga/sequencers|/ycga-gpfs/sequencers)', 's3://hpc-rsha-std-mult-000-a2/172.18.4.146/kc/.17712/data/1408/artico/archive', tfile)
            ret=downloadObj(objpath)
            if ret==0: return ret
            logger.info("No archive found")
            return 1
        else:
            logger.debug("failed")
            return 0
    else:
        logger.info("No tar archive found")
        return 1

def downloadObj(objpath):
    if o.dryrun:
        debugcmd= f"aws --cli-connect-timeout=300 --endpoint-url=https://s3.us-east-1.wasabisys.com s3 ls {objpath}"
        cmd=f"{aws_keys} {debugcmd}"
    else:
        debugcmd= f"aws --cli-connect-timeout=300 --endpoint-url=https://s3.us-east-1.wasabisys.com s3 cp {objpath} {o.dest}/"
        cmd=f"{aws_keys} {debugcmd}"

    logger.info(f"Found archived file\nDoing {debugcmd}\nPlease be patient")
    ret=subprocess.call(cmd, shell=True)
    logger.debug(f"aws cmd returned {ret}")
    return ret

if __name__=='__main__':
    parser=argparse.ArgumentParser(epilog=epilog, formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("url")
    parser.add_argument("-c", "--copy", dest="copy", action="store_true", default=False, help="force copy")
    parser.add_argument("-d", "--dest", dest="dest", default=None, help="destination")
    parser.add_argument("-n", "--dryrun", dest="dryrun", action="store_true", default=False, help="don't actually do anything")
    parser.add_argument("-v", "--verbose", dest="verbose", action="store_true", default=False, help="be verbose")
    o=parser.parse_args()
    
    # set up logging
    logger=logging.getLogger('archive')
    formatter=logging.Formatter("%(asctime)s %(levelname)s %(lineno)s %(message)s")
    logger.setLevel(logging.DEBUG if o.verbose else logging.INFO)
    hc=logging.StreamHandler()
    hc.setFormatter(formatter)
    logger.addHandler(hc)
    logger.debug(o)


    # log each request
    logfile=open('/gpfs/ycga/project/lsprog/tools/external/logs/URLFetch.log', 'a')
    start = datetime.datetime.now() # current date and time
    logfile.write(f'{start.strftime("%m/%d/%Y, %H:%M:%S")}: {os.getuid()} made request to retrieve {o.url}\n')
    logfile.flush()
    
    mo=urlpat.match(o.url)
    if not mo:
        fatal(f"Bad URL: {o.url}")

    randomdir, staging = mo.groups()
    ExtLinkDir=os.path.join(ELData, randomdir, staging)
    logger.debug(f"ExtLinkDir {ExtLinkDir}")
    if not os.path.islink(ExtLinkDir):
        fatal(f"ExtLinkDir non-Existent: {ExtLinkDir}")

    StageDir=os.readlink(ExtLinkDir)
    logger.debug(f"StageDir {StageDir}")
    if not os.path.isdir(StageDir):
        fatal(f"StageDir non-existent: {StageDir}")

    if not o.dest:
        o.dest=os.path.basename(StageDir)
    
    # check that contents of stagedir, which most likely are links, actually exist
    ok=checkForBroken(StageDir)
    logger.debug(f"StageDir ok: {ok}")

    if ok and not o.copy:
        # we'll just make a link and call it a day
        logger.info(f"Making links here: {o.dest}")
        logger.debug(f"symlink {ExtLinkDir} {o.dest}")
        if not o.dryrun:
            try:
                os.symlink(ExtLinkDir, o.dest)
            except FileExistsError:
                logger.error(f"{o.dest} exists, cannot create link.  Delete it, or chose different destination using -d")
    elif ok and o.copy:
        cmd=f'rsync {"--progress" if o.verbose else ""} -atL {StageDir}/ {o.dest}'
        logger.info(f"Doing a local rsync, please be patient")
        logger.debug(cmd)
        if not o.dryrun:
            ret=os.system(cmd)
        else:
            ret=0
        if ret!=0: fatal("rsync failed")
    else:
        # need to figure out where archive is, pull tarball, and untar it
        # find a link in the stagedir, remove the last element, and cross our fingers
        testlink=None
        for l in os.listdir(StageDir):
            fn=os.path.join(StageDir, l)
            if os.path.islink(fn):
                testlink=fn
                logger.debug(f"found link {testlink}")
                break
            
        if testlink:
            testtarget=os.readlink(testlink)
            logger.debug(f"looking for {testtarget}")
            ret=getTarball(testtarget)
        else:
            logger.info(f"No link found, giving up.")
            

        '''
        tarball=os.path.basename(object)
        logger.info(f"Doing an S3 download of {tarball} to {o.dest}, please be patient")
        if not o.dryrun: ObjectDownload(object, os.path.join(o.dest, tarball))
        '''

    now = datetime.datetime.now() # current date and time
    logfile.write(f'{now.strftime("%m/%d/%Y, %H:%M:%S")}: {os.getuid()} finished request to retrieve {o.url}.  time {now-start}. \n')
    logfile.flush()

